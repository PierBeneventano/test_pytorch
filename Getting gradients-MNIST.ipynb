{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('tiger_files/pb29/checkpoint/final/FINAL_dataset_MNIST-model_CNN-epoch_200-label_noise_prob_0-input_gaussian_noise_0-gaussian_noise_SD_0-noise_decay_fixed-batch_size_1024-lr_1.0.pt', map_location ='cpu')\n",
    "\n",
    "b = torch.load('tiger_files/pb29/checkpoint/final/FINAL_dataset_MNIST-model_CNN-epoch_200-label_noise_prob_0.2-input_gaussian_noise_0-gaussian_noise_SD_0-noise_decay_fixed-batch_size_1024-lr_1.0.pt', map_location ='cpu')\n",
    "\n",
    "c = torch.load('tiger_files/pb29/checkpoint/final/FINAL_dataset_MNIST-model_CNN-epoch_200-label_noise_prob_0.5-input_gaussian_noise_0-gaussian_noise_SD_0-noise_decay_fixed-batch_size_1024-lr_1.0.pt', map_location ='cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 98.77 98.78 98.79 \n",
      "\n",
      "train loss: 5.59257747605443e-05 5.583575405180454e-05 5.5548732168972495e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([95.9 , 97.94, 98.25, 98.5 , 98.56, 98.72, 98.74, 98.7 , 98.71,\n",
       "       98.77, 98.79, 98.78, 98.78, 98.8 , 98.79, 98.78, 98.78, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79, 98.79,\n",
       "       98.79, 98.79])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('test accuracy:', a['test_acc_array'][-1], b['test_acc_array'][-1], c['test_acc_array'][-1], '\\n')\n",
    "print('train loss:', a['train_loss_array'][-1], b['train_loss_array'][-1], c['train_loss_array'][-1])\n",
    "\n",
    "c['test_acc_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Loading the nets\n",
    "\n",
    "# net1 = VGG('VGG19')\n",
    "# net1.load_state_dict(a['net'], strict=False)\n",
    "# net1.eval()\n",
    "\n",
    "# net2 = VGG('VGG19')\n",
    "# net2.load_state_dict(b['net'], strict=False)\n",
    "# net2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_mnist(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the nets\n",
    "\n",
    "net1 = CNN_mnist()\n",
    "net1.load_state_dict(a['net'], strict=False)\n",
    "net1.eval()\n",
    "\n",
    "net2 = CNN_mnist()\n",
    "net2.load_state_dict(b['net'], strict=False)\n",
    "net2.eval()\n",
    "\n",
    "net3 = CNN_mnist()\n",
    "net3.load_state_dict(c['net'], strict=False)\n",
    "net3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the min entry in the example 0 is tensor(-0.4242)\n",
      "the max entry in the example 0 is tensor(2.8215)\n",
      "the min entry in the example 1 is tensor(-0.4242)\n",
      "the max entry in the example 1 is tensor(2.8215)\n",
      "the min entry in the example 2 is tensor(-0.4242)\n",
      "the max entry in the example 2 is tensor(2.8215)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the data\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "dataset1 = torchvision.datasets.MNIST('data', train=True, download=False,\n",
    "                       transform=transform)\n",
    "dataset2 = torchvision.datasets.MNIST('data', train=False,\n",
    "                       transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(dataset1, batch_size = 128)\n",
    "testloader = torch.utils.data.DataLoader(dataset2, batch_size = 1000)\n",
    "\n",
    "# plot 6 examples of data point\n",
    "examples = enumerate(trainloader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape\n",
    "\n",
    "for i in range(3):\n",
    "  print('the min entry in the example', i , 'is', torch.min(example_data[i][0]))\n",
    "  print('the max entry in the example', i , 'is', torch.max(example_data[i][0]))\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(3):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig\n",
    "\n",
    "example_data[0:1,:,:,:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jac_1 = {}\n",
    "Jac_2 = {}\n",
    "Jac_3 = {}\n",
    "# Jac_diff = {}\n",
    "\n",
    "for i in range(20):\n",
    "    Jac_1[i] = torch.autograd.functional.jacobian(net1, example_data[i:i+1,:,:,:], create_graph = True)\n",
    "    Jac_2[i] = torch.autograd.functional.jacobian(net2, example_data[i:i+1,:,:,:], create_graph = True)\n",
    "    Jac_3[i] = torch.autograd.functional.jacobian(net3, example_data[i:i+1,:,:,:], create_graph = True)\n",
    "#     Jac_diff[i] = torch.autograd.functional.jacobian(net_diff, example_data[i:i+1,:,:,:], create_graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the datapoint 0 we have (1/2) tensor(0.9977, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 1 we have (1/2) tensor(0.9793, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 2 we have (1/2) tensor(1.0071, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 3 we have (1/2) tensor(1.0081, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 4 we have (1/2) tensor(0.9991, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 5 we have (1/2) tensor(1.0026, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 6 we have (1/2) tensor(0.9750, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 7 we have (1/2) tensor(0.9943, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 8 we have (1/2) tensor(1.0513, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 9 we have (1/2) tensor(0.9992, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 10 we have (1/2) tensor(1.0016, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 11 we have (1/2) tensor(1.0038, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 12 we have (1/2) tensor(1.0053, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 13 we have (1/2) tensor(1.0069, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 14 we have (1/2) tensor(1.0044, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 15 we have (1/2) tensor(1.0101, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 16 we have (1/2) tensor(1.0101, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 17 we have (1/2) tensor(1.0283, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 18 we have (1/2) tensor(0.9967, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 19 we have (1/2) tensor(1.0141, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 0 we have (1/3) tensor(1.0010, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 1 we have (1/3) tensor(0.9827, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 2 we have (1/3) tensor(0.9963, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 3 we have (1/3) tensor(1.0086, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 4 we have (1/3) tensor(0.9766, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 5 we have (1/3) tensor(1.0386, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 6 we have (1/3) tensor(0.9658, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 7 we have (1/3) tensor(0.9852, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 8 we have (1/3) tensor(1.0033, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 9 we have (1/3) tensor(1.0024, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 10 we have (1/3) tensor(0.9928, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 11 we have (1/3) tensor(0.9948, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 12 we have (1/3) tensor(0.9982, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 13 we have (1/3) tensor(1.0029, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 14 we have (1/3) tensor(0.9964, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 15 we have (1/3) tensor(1.0491, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 16 we have (1/3) tensor(1.0286, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 17 we have (1/3) tensor(1.0165, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 18 we have (1/3) tensor(0.9915, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 19 we have (1/3) tensor(1.0022, grad_fn=<DivBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "#     print('on the datapoint', i, 'the Frobenius norm of the Jacobian of the SGD-trained net is:', torch.norm((Jac_1[i]), p='fro'), 'The one of the label noised:', torch.norm((Jac_2[i])))\n",
    "    print('on the datapoint', i, 'we have (1/2)', (torch.norm((Jac_1[i])) / torch.norm((Jac_2[i]))), '\\n')\n",
    "\n",
    "for i in range(20):\n",
    "    print('on the datapoint', i, 'we have (1/3)', (torch.norm((Jac_1[i])) / torch.norm((Jac_3[i]))), '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the datapoint 0 the difference of the two Jacobians is (1-2)/1 tensor(0.0968, grad_fn=<DivBackward0>)\n",
      "on the datapoint 1 the difference of the two Jacobians is (1-2)/1 tensor(0.0994, grad_fn=<DivBackward0>)\n",
      "on the datapoint 2 the difference of the two Jacobians is (1-2)/1 tensor(0.0734, grad_fn=<DivBackward0>)\n",
      "on the datapoint 3 the difference of the two Jacobians is (1-2)/1 tensor(0.0618, grad_fn=<DivBackward0>)\n",
      "on the datapoint 4 the difference of the two Jacobians is (1-2)/1 tensor(0.0556, grad_fn=<DivBackward0>)\n",
      "on the datapoint 5 the difference of the two Jacobians is (1-2)/1 tensor(0.0714, grad_fn=<DivBackward0>)\n",
      "on the datapoint 6 the difference of the two Jacobians is (1-2)/1 tensor(0.0950, grad_fn=<DivBackward0>)\n",
      "on the datapoint 7 the difference of the two Jacobians is (1-2)/1 tensor(0.0617, grad_fn=<DivBackward0>)\n",
      "on the datapoint 8 the difference of the two Jacobians is (1-2)/1 tensor(0.1639, grad_fn=<DivBackward0>)\n",
      "on the datapoint 9 the difference of the two Jacobians is (1-2)/1 tensor(0.0416, grad_fn=<DivBackward0>)\n",
      "on the datapoint 10 the difference of the two Jacobians is (1-2)/1 tensor(0.0584, grad_fn=<DivBackward0>)\n",
      "on the datapoint 11 the difference of the two Jacobians is (1-2)/1 tensor(0.0963, grad_fn=<DivBackward0>)\n",
      "on the datapoint 12 the difference of the two Jacobians is (1-2)/1 tensor(0.0572, grad_fn=<DivBackward0>)\n",
      "on the datapoint 13 the difference of the two Jacobians is (1-2)/1 tensor(0.0489, grad_fn=<DivBackward0>)\n",
      "on the datapoint 14 the difference of the two Jacobians is (1-2)/1 tensor(0.0616, grad_fn=<DivBackward0>)\n",
      "on the datapoint 15 the difference of the two Jacobians is (1-2)/1 tensor(0.0671, grad_fn=<DivBackward0>)\n",
      "on the datapoint 16 the difference of the two Jacobians is (1-2)/1 tensor(0.0702, grad_fn=<DivBackward0>)\n",
      "on the datapoint 17 the difference of the two Jacobians is (1-2)/1 tensor(0.0981, grad_fn=<DivBackward0>)\n",
      "on the datapoint 18 the difference of the two Jacobians is (1-2)/1 tensor(0.0822, grad_fn=<DivBackward0>)\n",
      "on the datapoint 19 the difference of the two Jacobians is (1-2)/1 tensor(0.0601, grad_fn=<DivBackward0>)\n",
      "on the datapoint 0 the difference of the two Jacobians is (1-3)/1 tensor(0.0533, grad_fn=<DivBackward0>)\n",
      "on the datapoint 1 the difference of the two Jacobians is (1-3)/1 tensor(0.1041, grad_fn=<DivBackward0>)\n",
      "on the datapoint 2 the difference of the two Jacobians is (1-3)/1 tensor(0.1179, grad_fn=<DivBackward0>)\n",
      "on the datapoint 3 the difference of the two Jacobians is (1-3)/1 tensor(0.1042, grad_fn=<DivBackward0>)\n",
      "on the datapoint 4 the difference of the two Jacobians is (1-3)/1 tensor(0.0904, grad_fn=<DivBackward0>)\n",
      "on the datapoint 5 the difference of the two Jacobians is (1-3)/1 tensor(0.1231, grad_fn=<DivBackward0>)\n",
      "on the datapoint 6 the difference of the two Jacobians is (1-3)/1 tensor(0.0992, grad_fn=<DivBackward0>)\n",
      "on the datapoint 7 the difference of the two Jacobians is (1-3)/1 tensor(0.0692, grad_fn=<DivBackward0>)\n",
      "on the datapoint 8 the difference of the two Jacobians is (1-3)/1 tensor(0.0477, grad_fn=<DivBackward0>)\n",
      "on the datapoint 9 the difference of the two Jacobians is (1-3)/1 tensor(0.0452, grad_fn=<DivBackward0>)\n",
      "on the datapoint 10 the difference of the two Jacobians is (1-3)/1 tensor(0.0930, grad_fn=<DivBackward0>)\n",
      "on the datapoint 11 the difference of the two Jacobians is (1-3)/1 tensor(0.0698, grad_fn=<DivBackward0>)\n",
      "on the datapoint 12 the difference of the two Jacobians is (1-3)/1 tensor(0.0441, grad_fn=<DivBackward0>)\n",
      "on the datapoint 13 the difference of the two Jacobians is (1-3)/1 tensor(0.0458, grad_fn=<DivBackward0>)\n",
      "on the datapoint 14 the difference of the two Jacobians is (1-3)/1 tensor(0.0615, grad_fn=<DivBackward0>)\n",
      "on the datapoint 15 the difference of the two Jacobians is (1-3)/1 tensor(0.1794, grad_fn=<DivBackward0>)\n",
      "on the datapoint 16 the difference of the two Jacobians is (1-3)/1 tensor(0.0934, grad_fn=<DivBackward0>)\n",
      "on the datapoint 17 the difference of the two Jacobians is (1-3)/1 tensor(0.0753, grad_fn=<DivBackward0>)\n",
      "on the datapoint 18 the difference of the two Jacobians is (1-3)/1 tensor(0.0685, grad_fn=<DivBackward0>)\n",
      "on the datapoint 19 the difference of the two Jacobians is (1-3)/1 tensor(0.0497, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "#     print('on the datapoint', i, 'the Frobenius norm of the Jacobian of the SGD-trained net is:', torch.norm((Jac_1[i]), p='fro'), 'The one of the label noised:', torch.norm((Jac_2[i])))\n",
    "    print('on the datapoint', i, 'the difference of the two Jacobians is (1-2)/1', torch.norm(Jac_1[i]-Jac_2[i])/torch.norm(Jac_1[i]))\n",
    "#     print('on the datapoint', i, 'the difference of the two neural nets is', torch.norm(net_diff(example_data[i:i+1,:,:,:]))/torch.norm(net1(example_data[i:i+1,:,:,:])))\n",
    "\n",
    "for i in range(20):\n",
    "    print('on the datapoint', i, 'the difference of the two Jacobians is (1-3)/1', torch.norm(Jac_1[i]-Jac_3[i])/torch.norm(Jac_1[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jac_4 = {}\n",
    "Jac_5 = {}\n",
    "Jac_6 = {}\n",
    "\n",
    "for i in range(20):\n",
    "    Jac_4[i] = torch.autograd.functional.jacobian(net1, example_data[i:i+1,:,:,:], create_graph = True)\n",
    "    Jac_5[i] = torch.autograd.functional.jacobian(net2, example_data[i:i+1,:,:,:], create_graph = True)\n",
    "    Jac_6[i] = torch.autograd.functional.jacobian(net3, example_data[i:i+1,:,:,:], create_graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the datapoint 0 we have (1/2) tensor(0.9977, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 1 we have (1/2) tensor(0.9793, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 2 we have (1/2) tensor(1.0071, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 3 we have (1/2) tensor(1.0081, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 4 we have (1/2) tensor(0.9991, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 5 we have (1/2) tensor(1.0026, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 6 we have (1/2) tensor(0.9750, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 7 we have (1/2) tensor(0.9943, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 8 we have (1/2) tensor(1.0513, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 9 we have (1/2) tensor(0.9992, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 10 we have (1/2) tensor(1.0016, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 11 we have (1/2) tensor(1.0038, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 12 we have (1/2) tensor(1.0053, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 13 we have (1/2) tensor(1.0069, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 14 we have (1/2) tensor(1.0044, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 15 we have (1/2) tensor(1.0101, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 16 we have (1/2) tensor(1.0101, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 17 we have (1/2) tensor(1.0283, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 18 we have (1/2) tensor(0.9967, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 19 we have (1/2) tensor(1.0141, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 0 we have (1/3) tensor(1.0010, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 1 we have (1/3) tensor(0.9827, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 2 we have (1/3) tensor(0.9963, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 3 we have (1/3) tensor(1.0086, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 4 we have (1/3) tensor(0.9766, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 5 we have (1/3) tensor(1.0386, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 6 we have (1/3) tensor(0.9658, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 7 we have (1/3) tensor(0.9852, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 8 we have (1/3) tensor(1.0033, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 9 we have (1/3) tensor(1.0024, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 10 we have (1/3) tensor(0.9928, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 11 we have (1/3) tensor(0.9948, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 12 we have (1/3) tensor(0.9982, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 13 we have (1/3) tensor(1.0029, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 14 we have (1/3) tensor(0.9964, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 15 we have (1/3) tensor(1.0491, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 16 we have (1/3) tensor(1.0286, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 17 we have (1/3) tensor(1.0165, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 18 we have (1/3) tensor(0.9915, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 19 we have (1/3) tensor(1.0022, grad_fn=<DivBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(20):\n",
    "#     print('on the datapoint', i, 'the Frobenius norm of the Jacobian of the SGD-trained net is:', torch.norm((Jac_1[i]), p='fro'), 'The one of the label noised:', torch.norm((Jac_2[i])))\n",
    "    print('on the datapoint', i, 'we have (1/2)', (torch.norm((Jac_4[i])) / torch.norm((Jac_5[i]))), '\\n')\n",
    "\n",
    "for i in range(20):\n",
    "    print('on the datapoint', i, 'we have (1/3)', (torch.norm((Jac_4[i])) / torch.norm((Jac_6[i]))), '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toplot = {\n",
    "    'train - 0.2': {'color': 'C0'},\n",
    "    'test - 0.2': { 'color': \"C1\"},\n",
    "    'train - 0.5': {'color': 'C2'},\n",
    "    'test - 0.5': { 'color': \"C3\"},\n",
    "}\n",
    " \n",
    "results = {}\n",
    "\n",
    "for label, fields in toplot.items():\n",
    "        if label == 'train - 0.2':\n",
    "           # accuracy_adam = mean_ad\n",
    "            results[label] = v_3\n",
    "        if label == 'test - 0.2':\n",
    "          #  accuracy_da = mean\n",
    "            results[label] = v_1\n",
    "        if label == 'train - 0.5':\n",
    "           # accuracy_adam = mean_ad\n",
    "            results[label] = v_4\n",
    "        if label == 'test - 0.5':\n",
    "          #  accuracy_da = mean\n",
    "            results[label] = v_2\n",
    "            \n",
    "            \n",
    "accuracy = {}\n",
    "\n",
    "for label, fields in toplot.items():\n",
    "        if label == 'train - 0.2':\n",
    "           # accuracy_adam = mean_ad\n",
    "            accuracy[label] = ('Train loss '+ (b['train_loss_array'][-1].astype('str'))[:6])\n",
    "        if label == 'test - 0.2':\n",
    "          #  accuracy_da = mean\n",
    "            accuracy[label] = ('Test accuracy '+str(b['best_acc']))\n",
    "        if label == 'train - 0.5':\n",
    "           # accuracy_adam = mean_ad\n",
    "            accuracy[label] = ('Train loss '+(c['train_loss_array'][-1].astype('str'))[:6])\n",
    "        if label == 'test - 0.5':\n",
    "          #  accuracy_da = mean\n",
    "            accuracy[label] = ('Test accuracy '+str(b['best_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
