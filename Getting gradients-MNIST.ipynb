{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('tiger_files/pb29/checkpoint/final/synthetic_linear_net/epoch_100-label_noise_prob_0-input_gaussian_noise_SD_0-gaussian_noise_SD_0.002-noise_decay_fixed-batch_size_64-lr_1.0.pt', map_location ='cpu')\n",
    "\n",
    "b = torch.load('tiger_files/pb29/checkpoint/final/synthetic_linear_net/epoch_100-label_noise_prob_0.1-input_gaussian_noise_SD_0-gaussian_noise_SD_0-noise_decay_fixed-batch_size_64-lr_1.0.pt', map_location ='cpu')\n",
    "\n",
    "c = torch.load('tiger_files/pb29/checkpoint/final/synthetic_linear_net/epoch_100-label_noise_prob_0.5-input_gaussian_noise_SD_0-gaussian_noise_SD_0-noise_decay_fixed-batch_size_64-lr_1.0.pt', map_location ='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 93.38 93.38 93.38 \n",
      "\n",
      "train loss: 0.00047144899773411453 0.00047144899773411453 0.00047144899773411453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([63.15, 74.84, 81.25, 86.65, 87.82, 90.47, 91.18, 92.24, 92.59,\n",
       "       92.82, 93.15, 93.2 , 93.34, 93.46, 93.38, 93.3 , 93.36, 93.31,\n",
       "       93.34, 93.37, 93.36, 93.36, 93.37, 93.37, 93.36, 93.36, 93.37,\n",
       "       93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38,\n",
       "       93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38,\n",
       "       93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38,\n",
       "       93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38,\n",
       "       93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38,\n",
       "       93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38,\n",
       "       93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38,\n",
       "       93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38, 93.38,\n",
       "       93.38])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('test accuracy:', a['test_acc_array'][-1], b['test_acc_array'][-1], c['test_acc_array'][-1], '\\n')\n",
    "print('train loss:', a['train_loss_array'][-1], b['train_loss_array'][-1], c['train_loss_array'][-1])\n",
    "\n",
    "a['test_acc_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Loading the nets\n",
    "\n",
    "# net1 = VGG('VGG19')\n",
    "# net1.load_state_dict(a['net'], strict=False)\n",
    "# net1.eval()\n",
    "\n",
    "# net2 = VGG('VGG19')\n",
    "# net2.load_state_dict(b['net'], strict=False)\n",
    "# net2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear_mnist(\n",
       "  (l1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (l3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (l4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (l5): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (l6): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the nets\n",
    "\n",
    "class Linear_mnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Linear_mnist, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 512)\n",
    "        self.l2 = nn.Linear(512, 512)\n",
    "        self.l3 = nn.Linear(512, 256)\n",
    "        self.l4 = nn.Linear(256, 256)\n",
    "        self.l5 = nn.Linear(256, 128)\n",
    "        self.l6 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        # x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.max_pool2d(x, 2)\n",
    "        # x = self.dropout1(x)\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # x = self.fc1(x)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.dropout2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "        x = self.l6(x)\n",
    "        output = F.log_softmax(x, dim=0)\n",
    "        return output\n",
    "\n",
    "\n",
    "net1 = Linear_mnist()\n",
    "net1.load_state_dict(a['net'], strict=False)\n",
    "net1.eval()\n",
    "\n",
    "net2 = Linear_mnist()\n",
    "net2.load_state_dict(b['net'], strict=False)\n",
    "net2.eval()\n",
    "\n",
    "net3 = Linear_mnist()\n",
    "net3.load_state_dict(c['net'], strict=False)\n",
    "net3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "\n",
    "class LinearNetData(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_IDs, labels, inputs):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.inputs[ID]\n",
    "        y = self.labels[ID]\n",
    "\n",
    "\n",
    "\n",
    "dataset1 = torch.load('data/synthetic_linear_net/training.pt')\n",
    "dataset2 = torch.load('data/synthetic_linear_net/test.pt')\n",
    "\n",
    "\n",
    "# plot 6 examples of data point\n",
    "# a.shape\n",
    "\n",
    "# for i in range(3):\n",
    "#   print('the min entry in the example', i , 'is', torch.min(example_data[i][0]))\n",
    "#   print('the max entry in the example', i , 'is', torch.max(example_data[i][0]))\n",
    "\n",
    "# fig = plt.figure()\n",
    "# for i in range(3):\n",
    "#   plt.subplot(2,3,i+1)\n",
    "#   plt.tight_layout()\n",
    "#   plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "#   plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "#   plt.xticks([])\n",
    "#   plt.yticks([])\n",
    "# fig\n",
    "\n",
    "# a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.2085e+01, -5.6835e-02, -4.1429e+01, -1.1729e+01, -7.1978e+01,\n",
      "        -4.3244e+01, -2.8960e+00, -1.7896e+01, -2.0145e+01, -4.9749e+01],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([-28.1846,  33.8436,  -7.5283,  22.1712, -38.0771,  -9.3434,  31.0044,\n",
      "         16.0045,  13.7551, -15.8483], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = dataset1.inputs[1]\n",
    "\n",
    "x=net1.l1(x)\n",
    "x=net1.l2(x)\n",
    "x=net1.l3(x)\n",
    "x=net1.l4(x)\n",
    "x=net1.l5(x)\n",
    "x=net1.l6(x)\n",
    "y = x\n",
    "x = F.log_softmax(x,0)\n",
    "\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jac_1 = {}\n",
    "Jac_2 = {}\n",
    "Jac_3 = {}\n",
    "# Jac_diff = {}\n",
    "\n",
    "for i in range(1,21):\n",
    "    x = dataset1.inputs[i]\n",
    "    Jac_1[i] = torch.autograd.functional.jacobian(net1, x, create_graph = True)\n",
    "    Jac_2[i] = torch.autograd.functional.jacobian(net2, x, create_graph = True)\n",
    "    Jac_3[i] = torch.autograd.functional.jacobian(net3, x, create_graph = True)\n",
    "#     Jac_diff[i] = torch.autograd.functional.jacobian(net_diff, example_data[i:i+1,:,:,:], create_graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jac_1[0] == Jac_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the datapoint 1 we have (1/2) tensor(1.0152, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 2 we have (1/2) tensor(1.0134, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 3 we have (1/2) tensor(1.0138, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 4 we have (1/2) tensor(1.0105, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 5 we have (1/2) tensor(1.0137, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 6 we have (1/2) tensor(1.0123, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 7 we have (1/2) tensor(1.0077, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 8 we have (1/2) tensor(1.0089, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 9 we have (1/2) tensor(1.0063, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 10 we have (1/2) tensor(1.0087, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 11 we have (1/2) tensor(1.0132, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 12 we have (1/2) tensor(1.0063, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 13 we have (1/2) tensor(1.0123, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 14 we have (1/2) tensor(1.0105, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 15 we have (1/2) tensor(1.0151, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 16 we have (1/2) tensor(1.0103, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 17 we have (1/2) tensor(1.0123, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 18 we have (1/2) tensor(1.0089, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 19 we have (1/2) tensor(1.0086, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 20 we have (1/2) tensor(1.0113, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 1 we have (1/3) tensor(1.5143, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 2 we have (1/3) tensor(1.4886, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 3 we have (1/3) tensor(1.4908, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 4 we have (1/3) tensor(1.4868, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 5 we have (1/3) tensor(1.4834, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 6 we have (1/3) tensor(1.4849, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 7 we have (1/3) tensor(1.4996, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 8 we have (1/3) tensor(1.4897, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 9 we have (1/3) tensor(1.4808, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 10 we have (1/3) tensor(1.5292, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 11 we have (1/3) tensor(1.4910, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 12 we have (1/3) tensor(1.4827, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 13 we have (1/3) tensor(1.4841, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 14 we have (1/3) tensor(1.4882, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 15 we have (1/3) tensor(1.6531, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 16 we have (1/3) tensor(1.5444, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 17 we have (1/3) tensor(1.4841, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 18 we have (1/3) tensor(1.4895, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 19 we have (1/3) tensor(1.4882, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 20 we have (1/3) tensor(1.4871, grad_fn=<DivBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,21):\n",
    "#     print('on the datapoint', i, 'the Frobenius norm of the Jacobian of the SGD-trained net is:', torch.norm((Jac_1[i]), p='fro'), 'The one of the label noised:', torch.norm((Jac_2[i])))\n",
    "    print('on the datapoint', i, 'we have (1/2)', (torch.norm((Jac_1[i])) / torch.norm((Jac_2[i]))), '\\n')\n",
    "\n",
    "for i in range(1,21):\n",
    "    print('on the datapoint', i, 'we have (1/3)', (torch.norm((Jac_1[i])) / torch.norm((Jac_3[i]))), '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the datapoint 1 the difference of the two Jacobians is (1-2)/1 tensor(0.0175, grad_fn=<DivBackward0>)\n",
      "on the datapoint 2 the difference of the two Jacobians is (1-2)/1 tensor(0.0149, grad_fn=<DivBackward0>)\n",
      "on the datapoint 3 the difference of the two Jacobians is (1-2)/1 tensor(0.0152, grad_fn=<DivBackward0>)\n",
      "on the datapoint 4 the difference of the two Jacobians is (1-2)/1 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "on the datapoint 5 the difference of the two Jacobians is (1-2)/1 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "on the datapoint 6 the difference of the two Jacobians is (1-2)/1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "on the datapoint 7 the difference of the two Jacobians is (1-2)/1 tensor(0.0119, grad_fn=<DivBackward0>)\n",
      "on the datapoint 8 the difference of the two Jacobians is (1-2)/1 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "on the datapoint 9 the difference of the two Jacobians is (1-2)/1 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "on the datapoint 10 the difference of the two Jacobians is (1-2)/1 tensor(0.0135, grad_fn=<DivBackward0>)\n",
      "on the datapoint 11 the difference of the two Jacobians is (1-2)/1 tensor(0.0324, grad_fn=<DivBackward0>)\n",
      "on the datapoint 12 the difference of the two Jacobians is (1-2)/1 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "on the datapoint 13 the difference of the two Jacobians is (1-2)/1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "on the datapoint 14 the difference of the two Jacobians is (1-2)/1 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "on the datapoint 15 the difference of the two Jacobians is (1-2)/1 tensor(0.0183, grad_fn=<DivBackward0>)\n",
      "on the datapoint 16 the difference of the two Jacobians is (1-2)/1 tensor(0.0125, grad_fn=<DivBackward0>)\n",
      "on the datapoint 17 the difference of the two Jacobians is (1-2)/1 tensor(0.0138, grad_fn=<DivBackward0>)\n",
      "on the datapoint 18 the difference of the two Jacobians is (1-2)/1 tensor(0.0128, grad_fn=<DivBackward0>)\n",
      "on the datapoint 19 the difference of the two Jacobians is (1-2)/1 tensor(0.0121, grad_fn=<DivBackward0>)\n",
      "on the datapoint 20 the difference of the two Jacobians is (1-2)/1 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "on the datapoint 1 the difference of the two Jacobians is (1-3)/1 tensor(0.3409, grad_fn=<DivBackward0>)\n",
      "on the datapoint 2 the difference of the two Jacobians is (1-3)/1 tensor(0.3284, grad_fn=<DivBackward0>)\n",
      "on the datapoint 3 the difference of the two Jacobians is (1-3)/1 tensor(0.3294, grad_fn=<DivBackward0>)\n",
      "on the datapoint 4 the difference of the two Jacobians is (1-3)/1 tensor(0.3277, grad_fn=<DivBackward0>)\n",
      "on the datapoint 5 the difference of the two Jacobians is (1-3)/1 tensor(0.3260, grad_fn=<DivBackward0>)\n",
      "on the datapoint 6 the difference of the two Jacobians is (1-3)/1 tensor(0.3267, grad_fn=<DivBackward0>)\n",
      "on the datapoint 7 the difference of the two Jacobians is (1-3)/1 tensor(0.3340, grad_fn=<DivBackward0>)\n",
      "on the datapoint 8 the difference of the two Jacobians is (1-3)/1 tensor(0.3291, grad_fn=<DivBackward0>)\n",
      "on the datapoint 9 the difference of the two Jacobians is (1-3)/1 tensor(0.3252, grad_fn=<DivBackward0>)\n",
      "on the datapoint 10 the difference of the two Jacobians is (1-3)/1 tensor(0.3525, grad_fn=<DivBackward0>)\n",
      "on the datapoint 11 the difference of the two Jacobians is (1-3)/1 tensor(0.3907, grad_fn=<DivBackward0>)\n",
      "on the datapoint 12 the difference of the two Jacobians is (1-3)/1 tensor(0.3260, grad_fn=<DivBackward0>)\n",
      "on the datapoint 13 the difference of the two Jacobians is (1-3)/1 tensor(0.3264, grad_fn=<DivBackward0>)\n",
      "on the datapoint 14 the difference of the two Jacobians is (1-3)/1 tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "on the datapoint 15 the difference of the two Jacobians is (1-3)/1 tensor(0.4533, grad_fn=<DivBackward0>)\n",
      "on the datapoint 16 the difference of the two Jacobians is (1-3)/1 tensor(0.3570, grad_fn=<DivBackward0>)\n",
      "on the datapoint 17 the difference of the two Jacobians is (1-3)/1 tensor(0.3264, grad_fn=<DivBackward0>)\n",
      "on the datapoint 18 the difference of the two Jacobians is (1-3)/1 tensor(0.3290, grad_fn=<DivBackward0>)\n",
      "on the datapoint 19 the difference of the two Jacobians is (1-3)/1 tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "on the datapoint 20 the difference of the two Jacobians is (1-3)/1 tensor(0.3278, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,21):\n",
    "#     print('on the datapoint', i, 'the Frobenius norm of the Jacobian of the SGD-trained net is:', torch.norm((Jac_1[i]), p='fro'), 'The one of the label noised:', torch.norm((Jac_2[i])))\n",
    "    print('on the datapoint', i, 'the difference of the two Jacobians is (1-2)/1', torch.norm(Jac_1[i]-Jac_2[i])/torch.norm(Jac_1[i]))\n",
    "#     print('on the datapoint', i, 'the difference of the two neural nets is', torch.norm(net_diff(example_data[i:i+1,:,:,:]))/torch.norm(net1(example_data[i:i+1,:,:,:])))\n",
    "\n",
    "for i in range(1,21):\n",
    "    print('on the datapoint', i, 'the difference of the two Jacobians is (1-3)/1', torch.norm(Jac_1[i]-Jac_3[i])/torch.norm(Jac_1[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jac_4 = {}\n",
    "Jac_5 = {}\n",
    "Jac_6 = {}\n",
    "# Jac_diff = {}\n",
    "\n",
    "for i in range(1,21):\n",
    "    x = dataset2.inputs[i+50000]\n",
    "    Jac_4[i] = torch.autograd.functional.jacobian(net1, x, create_graph = True)\n",
    "    Jac_5[i] = torch.autograd.functional.jacobian(net2, x, create_graph = True)\n",
    "    Jac_6[i] = torch.autograd.functional.jacobian(net3, x, create_graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the datapoint 1 we have (1/2) tensor(1.0137, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 2 we have (1/2) tensor(1.0070, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 3 we have (1/2) tensor(1.0107, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 4 we have (1/2) tensor(1.0129, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 5 we have (1/2) tensor(1.0134, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 6 we have (1/2) tensor(1.0071, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 7 we have (1/2) tensor(1.0123, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 8 we have (1/2) tensor(1.0113, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 9 we have (1/2) tensor(1.0094, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 10 we have (1/2) tensor(1.0070, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 11 we have (1/2) tensor(1.0070, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 12 we have (1/2) tensor(1.0121, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 13 we have (1/2) tensor(1.0137, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 14 we have (1/2) tensor(1.0137, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 15 we have (1/2) tensor(1.0137, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 16 we have (1/2) tensor(1.0133, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 17 we have (1/2) tensor(1.0056, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 18 we have (1/2) tensor(1.0141, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 19 we have (1/2) tensor(1.0137, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 20 we have (1/2) tensor(1.0137, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 1 we have (1/3) tensor(1.4833, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 2 we have (1/3) tensor(1.4816, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 3 we have (1/3) tensor(1.4817, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 4 we have (1/3) tensor(1.5070, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 5 we have (1/3) tensor(1.5007, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 6 we have (1/3) tensor(1.5104, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 7 we have (1/3) tensor(1.4841, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 8 we have (1/3) tensor(1.4877, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 9 we have (1/3) tensor(1.4852, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 10 we have (1/3) tensor(1.4816, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 11 we have (1/3) tensor(1.4818, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 12 we have (1/3) tensor(1.5170, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 13 we have (1/3) tensor(1.4833, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 14 we have (1/3) tensor(1.4833, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 15 we have (1/3) tensor(1.4834, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 16 we have (1/3) tensor(1.5380, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 17 we have (1/3) tensor(1.5117, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 18 we have (1/3) tensor(1.4932, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 19 we have (1/3) tensor(1.5056, grad_fn=<DivBackward0>) \n",
      "\n",
      "on the datapoint 20 we have (1/3) tensor(1.4833, grad_fn=<DivBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,21):\n",
    "#     print('on the datapoint', i, 'the Frobenius norm of the Jacobian of the SGD-trained net is:', torch.norm((Jac_1[i]), p='fro'), 'The one of the label noised:', torch.norm((Jac_2[i])))\n",
    "    print('on the datapoint', i, 'we have (1/2)', (torch.norm((Jac_4[i])) / torch.norm((Jac_5[i]))), '\\n')\n",
    "\n",
    "for i in range(1,21):\n",
    "    print('on the datapoint', i, 'we have (1/3)', (torch.norm((Jac_4[i])) / torch.norm((Jac_6[i]))), '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
